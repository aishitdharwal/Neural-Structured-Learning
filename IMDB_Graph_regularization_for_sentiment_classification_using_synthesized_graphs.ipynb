{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "IMDB - Graph regularization for sentiment classification using synthesized graphs",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aishitdharwal/Neural-Structured-Learning/blob/master/IMDB_Graph_regularization_for_sentiment_classification_using_synthesized_graphs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-jtAmwZWmYC",
        "colab_type": "text"
      },
      "source": [
        "## **Classify reviews as positive or negative**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN7V7L6xWtE1",
        "colab_type": "text"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uVnjPmOaQlnH",
        "colab": {}
      },
      "source": [
        "!pip install --quiet neural-structured-learning\n",
        "!pip install --quiet tensorflow-hub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2ew7HTbPpCJH",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import neural_structured_learning as nsl\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# Resets notebook state\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\n",
        "    \"GPU is\",\n",
        "    \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I35g4drW7Zo",
        "colab_type": "text"
      },
      "source": [
        "* Get data\n",
        "\n",
        "* Data: 25k training, 25k test reviews \n",
        "(equal number of positive and negative reviews in both)\n",
        "\n",
        "* num_words=10000 keeps the 10000 most frequently occurring words\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zXXx5Oc3pOmN",
        "colab": {}
      },
      "source": [
        "imdb = tf.keras.datasets.imdb\n",
        "(x_train, y_train), (x_test, y_test) = (imdb.load_data(num_words=10000))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y8qCnve_-lkO",
        "colab": {}
      },
      "source": [
        "print('Training entries: {}, labels: {}'.format(\n",
        "    len(x_train), len(y_train)))\n",
        "training_samples_count = len(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QtTS4kpEpjbi",
        "colab": {}
      },
      "source": [
        "print(x_train[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X-6Ii9Pfx6Nr",
        "colab": {}
      },
      "source": [
        "len(x_train[0]), len(x_train[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIQN9KmgX1VJ",
        "colab_type": "text"
      },
      "source": [
        "*   get the words and their pre-assigned index\n",
        "*   the indices 0,1,2,3 are reserved in the data, have to be prefixed by the following pre-assigned values\n",
        "*   Reversing the sorted_word_index key->value pair"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tr5s_1alpzop",
        "colab": {}
      },
      "source": [
        "def build_reverse_word_index():\n",
        "  # A dictionary mapping words to an integer index\n",
        "  word_index = imdb.get_word_index()\n",
        "\n",
        "  # The first indices are reserved\n",
        "  word_index = {k: (v + 3) for k, v in word_index.items()}\n",
        "  word_index['<PAD>'] = 0\n",
        "  word_index['<START>'] = 1\n",
        "  word_index['<UNK>'] = 2  # unknown\n",
        "  word_index['<UNUSED>'] = 3\n",
        "  return dict((value, key) for (key, value) in word_index.items())\n",
        "\n",
        "reverse_word_index = build_reverse_word_index()\n",
        "\n",
        "def decode_review(text):\n",
        "  return ' '.join([reverse_word_index.get(i, '?') for i in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s_OqxmH6-lkn",
        "colab": {}
      },
      "source": [
        "decode_review(x_train[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wZicFxFOeL2J",
        "colab": {}
      },
      "source": [
        "!mkdir -p /tmp/imdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKBdQqfaYKpb",
        "colab_type": "text"
      },
      "source": [
        "* Using pretrained Swivel embeddings\n",
        "\n",
        "* tf.reshape(x, shape=[-1]) flattens the array x\n",
        "\n",
        "* swivel embedding converts text to a 20 element vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nq2Ohd9CuZv_",
        "colab": {}
      },
      "source": [
        "pretrained_embedding = 'https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1'\n",
        "\n",
        "hub_layer = hub.KerasLayer(\n",
        "    pretrained_embedding, input_shape=[], dtype=tf.string, trainable=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lC-c1NdjYZlj",
        "colab_type": "text"
      },
      "source": [
        "* convert vectors into tf compatible format\n",
        "\n",
        "* tf.train.Feature wraps a list of data of a specific type so Tensorflow can understand it. It has a single attribute, which is a union of bytes_list/float_list/int64_list. Being a union, the stored list can be of type tf.train.BytesList (attribute name bytes_list), tf.train.FloatList (attribute name float_list), or tf.train.Int64List (attribute name int64_list).\n",
        "\n",
        "* create_embeddings() converts the training data into swivel embeddings and saves them as a feature in tfr (tensorflow record) format (easier and lighter to read by tensorflow)\n",
        "\n",
        "* the output 25000 is just the number of features recorded (returned as record_id)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wXJ3RaboTSKQ",
        "colab": {}
      },
      "source": [
        "def _int64_feature(value):\n",
        "  \"\"\"Returns int64 tf.train.Feature.\"\"\"\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value=value.tolist()))\n",
        "\n",
        "\n",
        "def _bytes_feature(value):\n",
        "  \"\"\"Returns bytes tf.train.Feature.\"\"\"\n",
        "  return tf.train.Feature(\n",
        "      bytes_list=tf.train.BytesList(value=[value.encode('utf-8')]))\n",
        "\n",
        "\n",
        "def _float_feature(value):\n",
        "  \"\"\"Returns float tf.train.Feature.\"\"\"\n",
        "  return tf.train.Feature(float_list=tf.train.FloatList(value=value.tolist()))\n",
        "\n",
        "\n",
        "def create_embedding_example(word_vector, record_id):\n",
        "  \"\"\"Create tf.Example containing the sample's embedding and its ID.\"\"\"\n",
        "\n",
        "  text = decode_review(word_vector)\n",
        "\n",
        "  # Shape = [batch_size,].\n",
        "  sentence_embedding = hub_layer(tf.reshape(text, shape=[-1,]))\n",
        "\n",
        "  # Flatten the sentence embedding back to 1-D.\n",
        "  sentence_embedding = tf.reshape(sentence_embedding, shape=[-1])\n",
        "\n",
        "  features = {\n",
        "      'id': _bytes_feature(str(record_id)),\n",
        "      'embedding': _float_feature(sentence_embedding.numpy())\n",
        "  }\n",
        "  return tf.train.Example(features=tf.train.Features(feature=features))\n",
        "\n",
        "\n",
        "def create_embeddings(word_vectors, output_path, starting_record_id):\n",
        "  record_id = int(starting_record_id)\n",
        "  with tf.io.TFRecordWriter(output_path) as writer:\n",
        "    for word_vector in word_vectors:\n",
        "      example = create_embedding_example(word_vector, record_id)\n",
        "      record_id = record_id + 1\n",
        "      writer.write(example.SerializeToString())\n",
        "  return record_id\n",
        "\n",
        "\n",
        "# Persist TF.Example features containing embeddings for training data in\n",
        "# TFRecord format.\n",
        "create_embeddings(x_train, '/tmp/imdb/embeddings.tfr', 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOoFXoygYyRE",
        "colab_type": "text"
      },
      "source": [
        "* create a graph from the features using nsl and save it in .tsv format\n",
        "\n",
        "* this uses cosine similarity as the metric to compare the embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DY6lqhNkBh2Q",
        "colab": {}
      },
      "source": [
        "nsl.tools.build_graph(['/tmp/imdb/embeddings.tfr'],\n",
        "                      '/tmp/imdb/graph_99.tsv',\n",
        "                      similarity_threshold=0.99)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiTPwxcXY6pd",
        "colab_type": "text"
      },
      "source": [
        "* the graph can be read in Pandas\n",
        "\n",
        "* For a similarity threshold=99%, there are 863786 edges created for the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFph6SWUY0nB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('/tmp/imdb/graph_99.tsv',sep='\\t')\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlw8DfPrZBtn",
        "colab_type": "text"
      },
      "source": [
        "* This creates a .tfr file of train, test data which has id, indices array of data, label of data\n",
        "\n",
        "* the output 50000 is just the number of data points covered (25k + 25k)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9PcUF4_B9grB",
        "colab": {}
      },
      "source": [
        "def create_example(word_vector, label, record_id):\n",
        "  \"\"\"Create tf.Example containing the sample's word vector, label, and ID.\"\"\"\n",
        "  features = {\n",
        "      'id': _bytes_feature(str(record_id)),\n",
        "      'words': _int64_feature(np.asarray(word_vector)),\n",
        "      'label': _int64_feature(np.asarray([label])),\n",
        "  }\n",
        "  return tf.train.Example(features=tf.train.Features(feature=features))\n",
        "\n",
        "def create_records(word_vectors, labels, record_path, starting_record_id):\n",
        "  record_id = int(starting_record_id)\n",
        "  with tf.io.TFRecordWriter(record_path) as writer:\n",
        "    for word_vector, label in zip(word_vectors, labels):\n",
        "      example = create_example(word_vector, label, record_id)\n",
        "      record_id = record_id + 1\n",
        "      writer.write(example.SerializeToString())\n",
        "  return record_id\n",
        "\n",
        "# Persist TF.Example features (word vectors and labels) for training and test\n",
        "# data in TFRecord format.\n",
        "next_record_id = create_records(x_train, y_train,\n",
        "                                '/tmp/imdb/train_data.tfr', 0)\n",
        "create_records(x_test, y_test, '/tmp/imdb/test_data.tfr',\n",
        "               next_record_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rhFO9sZ8Aa_g"
      },
      "source": [
        "* Pack the neighbors data (graph) with the training data, to create the augmented training data, with max. neighbors=3\n",
        "\n",
        "* Supports unlabeled data with labeled data (semi supervised learning), but it is kept blank for this problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lSCHj4rIBj_A",
        "colab": {}
      },
      "source": [
        "nsl.tools.pack_nbrs(\n",
        "    '/tmp/imdb/train_data.tfr',\n",
        "    '',\n",
        "    '/tmp/imdb/graph_99.tsv',\n",
        "    '/tmp/imdb/nsl_train_data.tfr',\n",
        "    add_undirected_edges=True,\n",
        "    max_nbrs=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AzBWdWkBqlMy"
      },
      "source": [
        "A class of hyperparameters for the model\n",
        "\n",
        "\n",
        "-   **num_classes**: There are 2 classes -- *positive* and *negative*.\n",
        "\n",
        "-   **max_seq_length**: This is the maximum number of words considered from each\n",
        "    movie review in this example.\n",
        "\n",
        "-   **vocab_size**: This is the size of the vocabulary considered for this\n",
        "    example.\n",
        "\n",
        "-   **distance_type**: This is the distance metric used to regularize the sample\n",
        "    with its neighbors.\n",
        "\n",
        "-   **graph_regularization_multiplier**: This controls the relative weight of\n",
        "    the graph regularization term in the overall loss function.\n",
        "\n",
        "-   **num_neighbors**: The number of neighbors used for graph regularization.\n",
        "    This value has to be less than or equal to the `max_nbrs` argument used\n",
        "    above when invoking `nsl.tools.pack_nbrs`.\n",
        "\n",
        "-   **num_fc_units**: The number of units in the fully connected layer of the\n",
        "    neural network.\n",
        "\n",
        "-   **train_epochs**: The number of training epochs.\n",
        "\n",
        "-   **batch_size**: Batch size used for training and evaluation.\n",
        "\n",
        "-   **eval_steps**: The number of batches to process before deeming evaluation\n",
        "    is complete. If set to `None`, all instances in the test set are evaluated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zsA8HuvvwGri",
        "colab": {}
      },
      "source": [
        "NBR_FEATURE_PREFIX = 'NL_nbr_'\n",
        "NBR_WEIGHT_SUFFIX = '_weight'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YlTmug7auQ2r",
        "colab": {}
      },
      "source": [
        "class HParams(object):\n",
        "  \"\"\"Hyperparameters used for training.\"\"\"\n",
        "  def __init__(self):\n",
        "    ### dataset parameters\n",
        "    self.num_classes = 2\n",
        "    self.max_seq_length = 256\n",
        "    self.vocab_size = 10000\n",
        "    ### neural graph learning parameters\n",
        "    self.distance_type = nsl.configs.DistanceType.L2\n",
        "    self.graph_regularization_multiplier = 0.1\n",
        "    self.num_neighbors = 2\n",
        "    ### model architecture\n",
        "    self.num_embedding_dims = 16\n",
        "    self.num_lstm_dims = 64\n",
        "    self.num_fc_units = 64\n",
        "    ### training parameters\n",
        "    self.train_epochs = 10\n",
        "    self.batch_size = 128\n",
        "    ### eval parameters\n",
        "    self.eval_steps = None  # All instances in the test set are evaluated.\n",
        "\n",
        "HPARAMS = HParams()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKoAOr9CZlPh",
        "colab_type": "text"
      },
      "source": [
        "* pad the input to make them of the same length\n",
        "\n",
        "* The graph regularization term has a different weight associated for different neighbors, setting them to 0 initially\n",
        "\n",
        "* map(map_func, num_parallel_calls=None, deterministic=None)\n",
        "\n",
        "Maps map_func across the elements of this dataset.\n",
        "\n",
        "This transformation applies map_func to each element of this dataset, and returns a new dataset containing the transformed elements, in the same order as they appeared in the input. map_func can be used to change both the values and the structure of a dataset's elements. For example, adding 1 to each element, or projecting a subset of element components.\n",
        "\n",
        "\n",
        "* create batches\n",
        "\n",
        ">> dataset = tf.data.Dataset.range(8)\n",
        "\n",
        ">> dataset = dataset.batch(3)\n",
        "\n",
        ">> list(dataset.as_numpy_iterator())\n",
        "\n",
        "[array([0,1,2]),array([3,4,5]),array([6,7])]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J5lkZVynuHWs",
        "colab": {}
      },
      "source": [
        "def pad_sequence(sequence, max_seq_length):\n",
        "  \"\"\"Pads the input sequence (a `tf.SparseTensor`) to `max_seq_length`.\"\"\"\n",
        "  pad_size = tf.maximum([0], max_seq_length - tf.shape(sequence)[0])\n",
        "  padded = tf.concat(\n",
        "      [sequence.values,\n",
        "       tf.fill((pad_size), tf.cast(0, sequence.dtype))],\n",
        "      axis=0)\n",
        "  # The input sequence may be larger than max_seq_length. Truncate down if\n",
        "  # necessary.\n",
        "  return tf.slice(padded, [0], [max_seq_length])\n",
        "\n",
        "def parse_example(example_proto):\n",
        "  \"\"\"Extracts relevant fields from the `example_proto`.\n",
        "\n",
        "  Args:\n",
        "    example_proto: An instance of `tf.train.Example`.\n",
        "\n",
        "  Returns:\n",
        "    A pair whose first value is a dictionary containing relevant features\n",
        "    and whose second value contains the ground truth labels.\n",
        "  \"\"\"\n",
        "  # The 'words' feature is a variable length word ID vector.\n",
        "  feature_spec = {\n",
        "      'words': tf.io.VarLenFeature(tf.int64),\n",
        "      'label': tf.io.FixedLenFeature((), tf.int64, default_value=-1),\n",
        "  }\n",
        "  # We also extract corresponding neighbor features in a similar manner to\n",
        "  # the features above.\n",
        "  for i in range(HPARAMS.num_neighbors):\n",
        "    nbr_feature_key = '{}{}_{}'.format(NBR_FEATURE_PREFIX, i, 'words')\n",
        "    nbr_weight_key = '{}{}{}'.format(NBR_FEATURE_PREFIX, i, NBR_WEIGHT_SUFFIX)\n",
        "    feature_spec[nbr_feature_key] = tf.io.VarLenFeature(tf.int64)\n",
        "\n",
        "    # We assign a default value of 0.0 for the neighbor weight so that\n",
        "    # graph regularization is done on samples based on their exact number\n",
        "    # of neighbors. In other words, non-existent neighbors are discounted.\n",
        "    feature_spec[nbr_weight_key] = tf.io.FixedLenFeature(\n",
        "        [1], tf.float32, default_value=tf.constant([0.0]))\n",
        "\n",
        "  features = tf.io.parse_single_example(example_proto, feature_spec)\n",
        "\n",
        "  # Since the 'words' feature is a variable length word vector, we pad it to a\n",
        "  # constant maximum length based on HPARAMS.max_seq_length\n",
        "  features['words'] = pad_sequence(features['words'], HPARAMS.max_seq_length)\n",
        "  for i in range(HPARAMS.num_neighbors):\n",
        "    nbr_feature_key = '{}{}_{}'.format(NBR_FEATURE_PREFIX, i, 'words')\n",
        "    features[nbr_feature_key] = pad_sequence(features[nbr_feature_key],\n",
        "                                             HPARAMS.max_seq_length)\n",
        "\n",
        "  labels = features.pop('label')\n",
        "  return features, labels\n",
        "\n",
        "def make_dataset(file_path, training=False):\n",
        "  \"\"\"Creates a `tf.data.TFRecordDataset`.\n",
        "\n",
        "  Args:\n",
        "    file_path: Name of the file in the `.tfrecord` format containing\n",
        "      `tf.train.Example` objects.\n",
        "    training: Boolean indicating if we are in training mode.\n",
        "\n",
        "  Returns:\n",
        "    An instance of `tf.data.TFRecordDataset` containing the `tf.train.Example`\n",
        "    objects.\n",
        "  \"\"\"\n",
        "  dataset = tf.data.TFRecordDataset([file_path])\n",
        "  if training:\n",
        "    dataset = dataset.shuffle(10000)\n",
        "  dataset = dataset.map(parse_example)\n",
        "  dataset = dataset.batch(HPARAMS.batch_size)\n",
        "  return dataset\n",
        "\n",
        "train_dataset = make_dataset('/tmp/imdb/nsl_train_data.tfr', True)\n",
        "test_dataset = make_dataset('/tmp/imdb/test_data.tfr')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LLC02j2g-llC"
      },
      "source": [
        "* Using bidirectional lstm model, since it is a sequence data, so RNN is apt for it. And words carry context far through the sentence, and lstm enables it to carry the context farther through the model. Without lstm, the effect of far away cells diminishes as we traverse through the neurons\n",
        "\n",
        "\n",
        "# Layers\n",
        "* inputs: takes the input of size max_seq_length (the integer encoded data) as specified in the hyperparamenter class\n",
        "\n",
        "* Embedding: takes the inputs layer and looks up the embedding for each word index, these embeddings are learned during training\n",
        "\n",
        "tf.keras.layers.Embedding(\n",
        "    input_dim,\n",
        "    output_dim,\n",
        "    embeddings_initializer=\"uniform\",\n",
        "    embeddings_regularizer=None,\n",
        "    activity_regularizer=None,\n",
        "    embeddings_constraint=None,\n",
        "    mask_zero=False,\n",
        "    input_length=None,\n",
        "    **kwargs\n",
        ")\n",
        "\n",
        "* LSTM: num_lstm_dims is the size of the output from the layer\n",
        "\n",
        "* Output: taking output as the probability of being a positive review, so size=1\n",
        "\n",
        "\n",
        "* Bidirectional layer doubles the number of parameters, so num_lstm_dims=64 gives 128 parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xpKOoWgu-llD",
        "colab": {}
      },
      "source": [
        "# # This function exists as an alternative to the bi-LSTM model used in this\n",
        "# # notebook.\n",
        "# def make_feed_forward_model():\n",
        "#   \"\"\"Builds a simple 2 layer feed forward neural network.\"\"\"\n",
        "#   inputs = tf.keras.Input(\n",
        "#       shape=(HPARAMS.max_seq_length,), dtype='int64', name='words')\n",
        "#   embedding_layer = tf.keras.layers.Embedding(HPARAMS.vocab_size, 16)(inputs)\n",
        "#   pooling_layer = tf.keras.layers.GlobalAveragePooling1D()(embedding_layer)\n",
        "#   dense_layer = tf.keras.layers.Dense(16, activation='relu')(pooling_layer)\n",
        "#   outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dense_layer)\n",
        "#   return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "\n",
        "def make_bilstm_model():\n",
        "  \"\"\"Builds a bi-directional LSTM model.\"\"\"\n",
        "  inputs = tf.keras.Input(\n",
        "      shape=(HPARAMS.max_seq_length,), dtype='int64', name='words')\n",
        "  embedding_layer = tf.keras.layers.Embedding(HPARAMS.vocab_size,\n",
        "                                              HPARAMS.num_embedding_dims)(\n",
        "                                                  inputs)\n",
        "  lstm_layer = tf.keras.layers.Bidirectional(\n",
        "      tf.keras.layers.LSTM(HPARAMS.num_lstm_dims))(\n",
        "          embedding_layer)\n",
        "  dense_layer = tf.keras.layers.Dense(\n",
        "      HPARAMS.num_fc_units, activation='relu')(\n",
        "          lstm_layer)\n",
        "  outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dense_layer)\n",
        "  return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "\n",
        "# Feel free to use an architecture of your choice.\n",
        "model = make_bilstm_model()\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L4EqVWg4-llM"
      },
      "source": [
        "since it is a binary classification problem, loss is taken as binary_crossentropy\n",
        "\n",
        "L = −(ylog(p)+(1−y)log(1−p))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Mr0GP-cQ-llN",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hCWYwkug-llQ"
      },
      "source": [
        "Create a validation set from training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oYTf7zkZQ-Dl",
        "colab": {}
      },
      "source": [
        "validation_fraction = 0.9\n",
        "validation_size = int(validation_fraction *\n",
        "                      int(training_samples_count / HPARAMS.batch_size))\n",
        "print(validation_size)\n",
        "validation_dataset = train_dataset.take(validation_size)\n",
        "train_dataset = train_dataset.skip(validation_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BLWzgfF1xpDu",
        "colab": {}
      },
      "source": [
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=HPARAMS.train_epochs,\n",
        "    verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6q7CoDfoCJ5h",
        "colab": {}
      },
      "source": [
        "results = model.evaluate(test_dataset, steps=HPARAMS.eval_steps)\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VcvSXvhp-llb",
        "colab": {}
      },
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nGoYf2Js-lle",
        "colab": {}
      },
      "source": [
        "acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"-r^\" is for solid red line with triangle markers.\n",
        "plt.plot(epochs, loss, '-r^', label='Training loss')\n",
        "# \"-b0\" is for solid blue line with circle markers.\n",
        "plt.plot(epochs, val_loss, '-bo', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6hXx-xOv-llh",
        "colab": {}
      },
      "source": [
        "plt.clf()   # clear figure\n",
        "\n",
        "plt.plot(epochs, acc, '-r^', label='Training acc')\n",
        "plt.plot(epochs, val_acc, '-bo', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pCIkVe_QFX38"
      },
      "source": [
        "# Graph Regularization on the above created model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WOEElnbtPzSr",
        "colab": {}
      },
      "source": [
        "# Build a new base LSTM model.\n",
        "base_reg_model = make_bilstm_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XGaDeyjEOMLC",
        "colab": {}
      },
      "source": [
        "# Wrap the base model with graph regularization.\n",
        "graph_reg_config = nsl.configs.make_graph_reg_config(\n",
        "    max_neighbors=HPARAMS.num_neighbors,\n",
        "    multiplier=HPARAMS.graph_regularization_multiplier,\n",
        "    distance_type=HPARAMS.distance_type,\n",
        "    sum_over_axis=-1)\n",
        "graph_reg_model = nsl.keras.GraphRegularization(base_reg_model,\n",
        "                                                graph_reg_config)\n",
        "graph_reg_model.compile(\n",
        "    optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aONZhwc9FWoo",
        "colab": {}
      },
      "source": [
        "graph_reg_history = graph_reg_model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=HPARAMS.train_epochs,\n",
        "    verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vdFMEfe2e5JY",
        "colab": {}
      },
      "source": [
        "graph_reg_results = graph_reg_model.evaluate(test_dataset, steps=HPARAMS.eval_steps)\n",
        "print(graph_reg_results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kHxshrYLah9v",
        "colab": {}
      },
      "source": [
        "graph_reg_history_dict = graph_reg_history.history\n",
        "graph_reg_history_dict.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YhjhH4n_aprb",
        "colab": {}
      },
      "source": [
        "acc = graph_reg_history_dict['accuracy']\n",
        "val_acc = graph_reg_history_dict['val_accuracy']\n",
        "loss = graph_reg_history_dict['loss']\n",
        "graph_loss = graph_reg_history_dict['graph_loss']\n",
        "val_loss = graph_reg_history_dict['val_loss']\n",
        "val_graph_loss = graph_reg_history_dict['val_graph_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.clf()   # clear figure\n",
        "\n",
        "# \"-r^\" is for solid red line with triangle markers.\n",
        "plt.plot(epochs, loss, '-r^', label='Training loss')\n",
        "# \"-gD\" is for solid green line with diamond markers.\n",
        "plt.plot(epochs, graph_loss, '-gD', label='Training graph loss')\n",
        "# \"-b0\" is for solid blue line with circle markers.\n",
        "plt.plot(epochs, val_loss, '-bo', label='Validation loss')\n",
        "# \"-ms\" is for solid magenta line with square markers.\n",
        "plt.plot(epochs, val_graph_loss, '-ms', label='Validation graph loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NE0vcDiqa1Id",
        "colab": {}
      },
      "source": [
        "plt.clf()   # clear figure\n",
        "\n",
        "plt.plot(epochs, acc, '-r^', label='Training acc')\n",
        "plt.plot(epochs, val_acc, '-bo', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}